{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package stopwords to /home/amy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note this notebook runs an HDP model to find the best number of topics. It is not strictly part of the flow and can be considered optional. It also uses a smaller sample of data to run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove new lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_df = pd.read_csv(\"current-tweets_cleaned.csv\")\n",
    "tweet_df = pd.read_csv(\"crawled_83k_cleaned.csv\")\n",
    "# the actual preprocessed text in the tweets\n",
    "tweets = tweet_df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tr = list(sent_to_words(tweet_df.text))\n",
    "#words_te = list(sent_to_words(tweet_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_tr[21][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tr = remove_stopwords(words_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    trigram = gensim.models.Phrases(bigram[words], min_count = tri_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return bigram_mod, trigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tr, trigram_tr = bigrams(words_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check some items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'go', 'around', 'corner', 'apartment', 'building', 'liz', 'miss', 'seeing', 'old', 'man', 'pee', 'sidewalk']\n",
      "<gensim.models.phrases.Phraser object at 0x7fbc385d15f8>\n"
     ]
    }
   ],
   "source": [
    "print(trigram_tr[bigram_tr[words_tr[7000]]][:200])\n",
    "print(bigram_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "#spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN','ADJ','VERB','ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test through trained model - will later run test data through trained model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_tr = [trigram_tr[bigram_tr[review]] for review in words_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lg = lemmatization(trigrams_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweets_lg.pkl', 'wb') as f:\n",
    "    pickle.dump(lemma_lg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note difference to un-lemmatized un-stop-worded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_lg[8811][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary and Corpus creation for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_lg = gensim.corpora.Dictionary(words_tr)\n",
    "id2word_lg.filter_extremes(no_below=10, no_above=0.35)\n",
    "id2word_lg.compactify()\n",
    "id2word_lg.save('train_dict_lg')\n",
    "corpus_lg = [id2word_lg.doc2bow(text) for text in words_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('83k_tweets_lg.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus_lg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lg[21][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2word_lg[id], freq) for id, freq in corpus_lg[21]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDP Model - auto-dinals the best number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "hdp = HdpModel(corpus_lg, id2word_lg, chunksize=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hdp.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*sword + 0.001*nudes + 0.001*smooth + 0.001*verse + 0.001*made + 0.001*mots + 0.001*kat + 0.001*playing + 0.001*cheese + 0.001*screenshots'),\n",
       " (1,\n",
       "  '0.002*tons + 0.002*pay + 0.001*blame + 0.001*conspiracy + 0.001*game + 0.001*dumbass + 0.001*trophy + 0.001*fear + 0.001*facts + 0.001*sue'),\n",
       " (2,\n",
       "  '0.002*sword + 0.001*promise + 0.001*negative + 0.001*thank + 0.001*govt + 0.001*scratch + 0.001*past + 0.001*eyes + 0.001*protecting + 0.001*omg'),\n",
       " (3,\n",
       "  '0.002*trees + 0.002*mobile + 0.001*circle + 0.001*goin + 0.001*challenge + 0.001*cross + 0.001*mercy + 0.001*history + 0.001*bright + 0.001*daughter'),\n",
       " (4,\n",
       "  '0.002*partner + 0.002*articles + 0.002*habit + 0.002*reminding + 0.002*masks + 0.001*completely + 0.001*friend + 0.001*make + 0.001*cream + 0.001*ateez'),\n",
       " (5,\n",
       "  '0.002*look + 0.002*mama + 0.001*receiving + 0.001*sweet + 0.001*ff + 0.001*shook + 0.001*match + 0.001*netflix + 0.001*billie + 0.001*monday'),\n",
       " (6,\n",
       "  '0.002*male + 0.001*wakes + 0.001*says + 0.001*dislike + 0.001*hashtag + 0.001*opinions + 0.001*ye + 0.001*owner + 0.001*rapper + 0.001*midnight'),\n",
       " (7,\n",
       "  '0.001*personality + 0.001*closing + 0.001*philly + 0.001*tomatoes + 0.001*pants + 0.001*songs + 0.001*bee + 0.001*anna + 0.001*finish + 0.001*guessing'),\n",
       " (8,\n",
       "  '0.002*summer + 0.002*shooting + 0.002*scotia + 0.001*godbless + 0.001*stopping + 0.001*lol + 0.001*funny + 0.001*im + 0.001*prob + 0.001*maintain'),\n",
       " (9,\n",
       "  '0.002*mee + 0.002*heartbreak + 0.001*wash + 0.001*museum + 0.001*stuck + 0.001*banks + 0.001*lmaooo + 0.001*worker + 0.001*american + 0.001*landed'),\n",
       " (10,\n",
       "  '0.002*ajax + 0.002*honey + 0.001*ends + 0.001*argh + 0.001*worst + 0.001*lip + 0.001*announce + 0.001*jaw + 0.001*chapter + 0.001*beard'),\n",
       " (11,\n",
       "  '0.002*ii + 0.002*kiya + 0.001*halloween + 0.001*eater + 0.001*hehehe + 0.001*outta + 0.001*spam + 0.001*consider + 0.001*miller + 0.001*dealing'),\n",
       " (12,\n",
       "  '0.002*mcu + 0.001*woah + 0.001*wat + 0.001*kisses + 0.001*conspiracy + 0.001*robert + 0.001*mothers + 0.001*puke + 0.001*starving + 0.001*shes'),\n",
       " (13,\n",
       "  '0.002*hungry + 0.002*uni + 0.002*wouldve + 0.001*deeply + 0.001*socialism + 0.001*injured + 0.001*birthday + 0.001*oppa + 0.001*yuh + 0.001*anyways'),\n",
       " (14,\n",
       "  '0.002*state + 0.002*yesterday + 0.002*otherwise + 0.001*adventure + 0.001*asshole + 0.001*favourite + 0.001*teach + 0.001*squad + 0.001*disrespect + 0.001*threatening'),\n",
       " (15,\n",
       "  '0.002*scored + 0.001*follow + 0.001*funny + 0.001*mots + 0.001*luke + 0.001*cooked + 0.001*puppy + 0.001*toronto + 0.001*impossible + 0.001*lil'),\n",
       " (16,\n",
       "  '0.002*flop + 0.002*active + 0.001*gas + 0.001*hugs + 0.001*trouble + 0.001*nominated + 0.001*bold + 0.001*leo + 0.001*trolls + 0.001*listen'),\n",
       " (17,\n",
       "  '0.002*metal + 0.001*delayed + 0.001*beaten + 0.001*mobile + 0.001*slip + 0.001*lip + 0.001*selection + 0.001*cross + 0.001*greet + 0.001*nuh'),\n",
       " (18,\n",
       "  '0.002*med + 0.002*niece + 0.001*completely + 0.001*es + 0.001*damage + 0.001*golf + 0.001*barr + 0.001*continues + 0.001*beg + 0.001*moved'),\n",
       " (19,\n",
       "  '0.001*yung + 0.001*doubt + 0.001*court + 0.001*liverpool + 0.001*breathing + 0.001*load + 0.001*committed + 0.001*glory + 0.001*roblox + 0.001*plays')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp.print_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
