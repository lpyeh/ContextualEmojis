{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package stopwords to /home/amy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import from Data Prep Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "75369    0\n",
      "75370    0\n",
      "75371    0\n",
      "75372    0\n",
      "75373    0\n",
      "Name: anger, Length: 75374, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#with open('../data/rev_train.pkl', 'rb') as f:\n",
    "#    rev_train = pickle.load(f)\n",
    "#with open('../data/rev_test.pkl', 'rb') as f:\n",
    "#    rev_test = pickle.load(f)\n",
    "#train_df = pd.read_csv('may-to-sept_cleaned.csv')\n",
    "train_df = pd.read_csv('crawled_83k_cleaned.csv')\n",
    "print(train_df['anger'])\n",
    "test_df = pd.read_csv('current-tweets_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Review Text for LDA - Need to make Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
    "                  'would','really','like','great','service','came','got'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lemmatization(texts, allowed_postags=['NOUN','ADV']):\n",
    "     texts_out = []\n",
    "     for sent in texts:\n",
    "         doc = nlp(\" \".join(sent)) \n",
    "         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    \"\"\"\n",
    "    Get Bigram Model, Corpus, id2word mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    df['text'] = strip_newline(df.text)\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "    lemma = lemmatization(bigram)\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_train4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well', 'marcom', 'pal']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus4, train_id2word4, bigram_train4 = get_corpus(train_df)\n",
    "bigram_train4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data Back in for New Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train_corpus4.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_corpus4, f)\n",
    "# with open('train_id2word4.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_id2word4, f)\n",
    "# with open('bigram_train4.pkl', 'wb') as f:\n",
    "#     pickle.dump(bigram_train4, f)\n",
    "# # with open('lemma_train.pkl', 'wb') as f:\n",
    "# #     pickle.dump(lemma_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make sure importing right model\n",
    "# lda_train4 = gensim.models.ldamulticore.LdaMulticore.load('lda_train4.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lg = None\n",
    "#with open('tweets_lg.pkl', 'rb') as f:\n",
    "with open('83k_tweets_lg.pkl', 'rb') as f:\n",
    "   corpus_lg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA Model in Gensim\n",
    "Note that running eval_every=1 does this in batches of chunksize batches. The 20 is chosen from a HDP Process Test on the same data in another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train4 = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus4,\n",
    "                           num_topics=20,\n",
    "                           id2word=train_id2word4,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "    lda_train4.save('lda_train4.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coherence Model Score seems useless but included here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5378 unique tokens: ['pal', 'well', 'ft', 'normal', 'size']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25271502714345184"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_id2word4)\n",
    "coherence_model_lda = CoherenceModel(model=lda_train4, texts=bigram_train4, dictionary=train_id2word4, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "coherence_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.215*\"love\" + 0.054*\"work\" + 0.039*\"something\" + 0.031*\"maybe\" + 0.030*\"done\" + 0.030*\"nice\" + 0.028*\"cause\" + 0.027*\"much\" + 0.025*\"keep\" + 0.022*\"making\" + 0.021*\"old\" + 0.021*\"true\" + 0.021*\"school\" + 0.021*\"guy\" + 0.019*\"night\"'),\n",
       " (1,\n",
       "  '0.095*\"need\" + 0.087*\"thank\" + 0.071*\"omg\" + 0.051*\"take\" + 0.046*\"wanna\" + 0.045*\"girl\" + 0.035*\"okay\" + 0.034*\"better\" + 0.034*\"cant\" + 0.034*\"give\" + 0.031*\"days\" + 0.028*\"away\" + 0.022*\"use\" + 0.021*\"name\" + 0.020*\"hit\"'),\n",
       " (2,\n",
       "  '0.046*\"stop\" + 0.046*\"thought\" + 0.040*\"youre\" + 0.037*\"years\" + 0.036*\"face\" + 0.036*\"find\" + 0.034*\"literally\" + 0.032*\"end\" + 0.032*\"bts\" + 0.031*\"soon\" + 0.028*\"didnt\" + 0.025*\"mind\" + 0.023*\"yall\" + 0.019*\"lot\" + 0.018*\"watched\"'),\n",
       " (3,\n",
       "  '0.191*\"one\" + 0.055*\"also\" + 0.038*\"fuck\" + 0.036*\"things\" + 0.035*\"long\" + 0.030*\"fucking\" + 0.028*\"god\" + 0.023*\"left\" + 0.021*\"hes\" + 0.021*\"day\" + 0.020*\"took\" + 0.018*\"called\" + 0.017*\"open\" + 0.017*\"eyes\" + 0.017*\"awesome\"'),\n",
       " (4,\n",
       "  '0.078*\"please\" + 0.050*\"already\" + 0.047*\"ur\" + 0.044*\"actually\" + 0.040*\"heart\" + 0.038*\"tell\" + 0.031*\"cat\" + 0.027*\"read\" + 0.023*\"kids\" + 0.022*\"world\" + 0.022*\"funny\" + 0.020*\"win\" + 0.017*\"pm\" + 0.017*\"know\" + 0.015*\"covid\"'),\n",
       " (5,\n",
       "  '0.124*\"back\" + 0.050*\"tomorrow\" + 0.046*\"home\" + 0.043*\"big\" + 0.037*\"tonight\" + 0.034*\"coming\" + 0.030*\"saw\" + 0.028*\"buy\" + 0.028*\"hey\" + 0.028*\"song\" + 0.027*\"must\" + 0.022*\"close\" + 0.021*\"check\" + 0.020*\"change\" + 0.019*\"dad\"'),\n",
       " (6,\n",
       "  '0.117*\"want\" + 0.090*\"think\" + 0.044*\"life\" + 0.039*\"friends\" + 0.038*\"real\" + 0.033*\"two\" + 0.031*\"twitter\" + 0.028*\"another\" + 0.027*\"fun\" + 0.026*\"whole\" + 0.026*\"video\" + 0.020*\"saying\" + 0.020*\"stay\" + 0.019*\"episode\" + 0.016*\"send\"'),\n",
       " (7,\n",
       "  '0.066*\"man\" + 0.064*\"know\" + 0.064*\"today\" + 0.059*\"lol\" + 0.052*\"first\" + 0.041*\"beautiful\" + 0.037*\"guys\" + 0.036*\"put\" + 0.029*\"hard\" + 0.029*\"amazing\" + 0.028*\"start\" + 0.027*\"pretty\" + 0.025*\"morning\" + 0.021*\"season\" + 0.020*\"hell\"'),\n",
       " (8,\n",
       "  '0.069*\"way\" + 0.060*\"wow\" + 0.056*\"thanks\" + 0.055*\"best\" + 0.042*\"ass\" + 0.025*\"guess\" + 0.023*\"looking\" + 0.023*\"sick\" + 0.023*\"call\" + 0.022*\"nigga\" + 0.022*\"gone\" + 0.022*\"idk\" + 0.022*\"month\" + 0.022*\"around\" + 0.019*\"free\"'),\n",
       " (9,\n",
       "  '0.065*\"new\" + 0.056*\"could\" + 0.045*\"hate\" + 0.036*\"sleep\" + 0.035*\"ready\" + 0.032*\"went\" + 0.030*\"ill\" + 0.030*\"nothing\" + 0.028*\"remember\" + 0.027*\"ive\" + 0.024*\"anyone\" + 0.023*\"rn\" + 0.020*\"mine\" + 0.017*\"knew\" + 0.017*\"run\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train4.print_topics(20,num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an LDA model, we need to run all the reviews through it using 'get document topics'. A list comprehension on that output (2nd line in loop) will give the probability distribution of the topics for a specific review, and that's our feature vector. The other 2 lines in the loop are hand-engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(train_df)):\n",
    "    top_topics = lda_train4.get_document_topics(train_corpus4[i], minimum_probability=0.0)\n",
    "    #print(train_corpus4[i])\n",
    "    topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "    #topic_vec.extend([train_df.iloc[i].real_counts]) # counts of reviews for restaurant\n",
    "    #topic_vec.extend([len(train_df.iloc[i].text)]) # length review\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016667739, 0.016667739, 0.016667739, 0.34996042, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.016667739, 0.35002026, 0.016667739]\n",
      "[0.010000642, 0.4099947, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.2100086, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.010000642, 0.2099858]\n",
      "[0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.02500058, 0.52498895, 0.02500058]\n"
     ]
    }
   ],
   "source": [
    "print(train_vecs[0])\n",
    "print(train_vecs[1])\n",
    "print(train_vecs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn needs np arrays for CV-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       anger  disgust  fear  happiness  sadness  surprise\n",
      "0          0        0     0          1        0         0\n",
      "1          0        0     0          1        0         0\n",
      "2          0        0     0          1        0         0\n",
      "3          0        0     0          1        0         0\n",
      "4          0        0     0          1        0         0\n",
      "...      ...      ...   ...        ...      ...       ...\n",
      "75369      0        0     0          0        1         0\n",
      "75370      0        0     0          0        1         0\n",
      "75371      0        0     0          0        0         1\n",
      "75372      0        0     0          0        0         1\n",
      "75373      0        0     0          0        0         0\n",
      "\n",
      "[75374 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#y = np.array(train_df[\"\".target)\n",
    "#print(train_df['anger'])\n",
    "y = train_df.loc[:,['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise'],] \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('y.pkl', 'wb') as f:\n",
    "    pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X.pkl', 'wb') as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SVM classifier to predict emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def multiClassSVM(vectors, df):\n",
    "    categories = ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']\n",
    "    X_train, X_test, y_train, y_test= train_test_split(vectors, df, random_state=42, test_size=0.33, shuffle=True)\n",
    "    #X_train = train.vectors\n",
    "    #X_test = test.vectors\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    SVC_pipeline = Pipeline([\n",
    "                    #('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                    ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "                ])\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for category in categories:\n",
    "        print('... Processing {}'.format(category))\n",
    "        # train the model using X_dtm & y\n",
    "        #print(X_train)\n",
    "        SVC_pipeline.fit(X_train, y_train[category])\n",
    "        # compute the testing accuracy\n",
    "        prediction = SVC_pipeline.predict(X_test)\n",
    "        #accuracy = accuracy_score(y_test[category], prediction)\n",
    "        accuracy = f1_score(y_true=y_test[category], y_pred=prediction, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "        predictions.extend(prediction)\n",
    "        labels.extend(y_test[category])\n",
    "        accuracies.append(accuracy)\n",
    "        #print('Test accuracy is {}'.format(accuracy))\n",
    "    accuracy = f1_score(y_true=labels, y_pred=predictions, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50500, 20)\n",
      "(24874, 20)\n",
      "... Processing anger\n",
      "... Processing disgust\n",
      "... Processing fear\n",
      "... Processing happiness\n",
      "... Processing sadness\n",
      "... Processing surprise\n",
      "0.24282895735316434\n"
     ]
    }
   ],
   "source": [
    "accuracy = multiClassSVM(X, train_df)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final 5-fold CV loop for training on 2016 review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    4 ... 9892 9893 9895]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([   1,    2,    4,    5,    6,    7,    9,   11,   13,   15,\\n            ...\\n            9882, 9883, 9884, 9887, 9888, 9890, 9891, 9892, 9893, 9895],\\n           dtype='int64', length=7916)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-1d979b479e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(val_ind)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([   1,    2,    4,    5,    6,    7,    9,   11,   13,   15,\\n            ...\\n            9882, 9883, 9884, 9887, 9888, 9890, 9891, 9892, 9893, 9895],\\n           dtype='int64', length=7916)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "#print(kf.split(X, y))\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    print(train_ind)\n",
    "    #print(val_ind)\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
